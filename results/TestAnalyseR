rm(list=ls())
install.packages(c("tidyr", "stringr", "dplyr", "quanteda", 
                   "quanteda.textstats", "ggplot2", "wordcloud", 
                   "wordcloud2", "RColorBrewer","topicmodels","LDAvis"))

library(tidyr)
library(stringr)
library(dplyr)
library(quanteda)
library(quanteda.textstats)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)

setwd("Downloads/ProjetIC05")
base<-read.csv2("2023tweet.csv",header=T, sep=",")

summary(base)

#Suppression des doublons

a<-as.vector(which(duplicated(base$Tweet.body)==T))
base<-base[-a,]

#Suppression des dates des pubs (1970)

base$Date<-as.Date(base$Date, "%Y-%m-%d")
base <- subset(base, format(base$Date, "%Y") != "1970")


#Graphique Tweet en fonction de la date

ggplot(base, aes(x=Date, y=..count..))+
  geom_bar()+
  theme_classic()

#Cr√©ation base auteur

base_auteurs<-as.data.frame(table(base$Handle))



##LEXICOMETRIE
#Cr√©ation du corpus

cp <- corpus(base$Tweet.body,
             docnames=base$Tweet.body, ## j'ai ajout√© cette ligne pour identifier les tweets
             docvars = select(base,Handle, Date, Username))

#tokens

tk<-tokens(cp, what = "word", remove_punct = TRUE, remove_numbers= T, remove_url=T)
dfm<-dfm(cp, tolower=T,remove_punct=T, remove_numbers=T, remove_url = T)

topfeatures(dfm, n=100) ##mots les plus fr√©quents

textstat_collocations(tk, min_count = 5, size = 2L) %>% 
  arrange(desc(count)) %>% 
  slice(1:50)    ###les ensembles de 2 mots qui sont les plus fr√©quents

### D√©finition d'un dictionnaire

dict<-dictionary(list(Education=c("√©ducation", "education", "#education","√©tudes","etude","etudes"), 
                      Triche=c("triches","tricherie","tricher"), 
                      Payant=c("payant", "payer","prix"),
                      gratuit=c("gratuit","gratuitement"),
                      humanite=c("humain","humanit√©","robot","robots"),
                      regulation=c("√©tats", "etat", "loi","gouvernement")))

## 3. Utiliser et enrichir la liste des stopwords 
stopwords("french")
stopwords("english")

toremove<-c(stopwords("french"), stopwords("english"),"‚Äô","a", "comme", "d'un", "d'une", "aussi", "fait", 
            "√™tre", "c'est", "an", "faire", "dire", "si", "qu'il", 
            "o√π", "tout", "plus", "encore", "d√©j√†", "depuis",
            "ans", "entre", "n'est", "peut", "dont", "donc", 
            "ainsi", "faut","va", "donc", "tous", "alors",
            "chez", "fois", "quand", "√©galement", "n'a", "n'y", 
            "celui", "celle", "l'un", "n'ont", 
            "l'a", "l'on","qu'on","or","d'ici","s'il","l√†", "d√®s",
            "dit","pu","six","pu","font","ceux","peut",
            "j'ai","ni","tr√®s", "lune", "lors", "puis", "etc", "tel", 
            "chaque", "ca", "veut", "toute", "quelle"
            ,"peu", "moin", "apr√®s", "bien", "deux", "trois", "oui",
            "avant", "√ßa", "sest", "notamment","tant","peuvent", 
            "selon", "quelque", "toujour", "avoir", "car", "beaucoup", 
            "sous", "non", "d'autre", "contre", "plusieurs", 
            "autre", "toute", "fin", "heure", 
            "lundi", "mardi", "mercredi", "jeudi", "vendredi", "samedi", "dimanche", 
            "dans", "pas", "me", "nos", "nous", "de", "vous", "sans", "mais", "d'accord",
            "voir", "parce", "dis", "dit", 'vont', "rien", "qu'ils", "quoi", "juste",
            "pourquoi", "trop", "peux", "moins", "depuis", "sous", "t'es", "ah", "vois", 
            "vais", "vraiment", "y'a", "vas", "bla", "e", "d'√™tre", "veux", "mois", "sen", 
            "bah", "regarde", "tiens", "compl√®tement", "completement", "sait", "ten", "vers", 
            "+", "toutes", "|", "via", "mettre", "in", "of", "üëâ", "üëá","‚û°","chatgpt","gpt","chat","comment","#chatgpt","j'ai","c‚Äôest",
            "voici","j‚Äôai","m'a","qu'il","GRATUIT","ia","l'ia","<",">","=","gratuit")

dfm<-dfm(cp, tolower=T,remove_punct=T, remove_numbers=T, remove_url = T, thesaurus=dict)
dfm<-dfm_remove(dfm, toremove)
topfeatures(dfm, n=100)


dfm<-dfm_remove(dfm, toremove)
tk<-tokens_remove(tk, toremove)

#'Gros pb avec le mot gratuit, ce sont des spams, et il y en a bcp bcp

##    A quels mots est associ√© chatgpt ici

gpt<-as.data.frame(kwic(cp, pattern="gratuit", window=10))



## 7.Tracer le nuage de mots avec la fonction wordcloud. 

par(mar=c(0,0,0,0))

wd<-as.data.frame(topfeatures(dfm, n=200))
wd$word<-rownames(wd)
colnames(wd)<-c("freq","word")

set.seed(1234)
wordcloud(words=wd$word, freq=wd$freq, random.order=FALSE, min.freq=30)

wd<-wd[,c("word","freq")]
wordcloud2(data=wd, size=1, color='random-dark')




### III. MODELES THEMATIQUES
library(topicmodels)

dtm <- quanteda::convert(dfm, to = "topicmodels")


## 2. Calcul du mod√®le (choisir la valeur de K)
res_lda <- LDA(dtm,
               k = 8, method = "Gibbs", 
               control = list(seed = 145, alpha = 0.6))

## 3. Lecture et interpr√©tation des r√©sultats 
terms(res_lda, 8)#### les principaux termes associ√©s √† chaque topic


posterior(res_lda)$topics ### la pr√©sence de chaque th√®me dans les commentaires
textes_topics<-as.data.frame(posterior(res_lda)$topics)
colnames(textes_topics)<-paste0("tp",1:8)
apply(textes_topics,2, mean)### Pr√©sence moyenne de nos topics dans le corpus
textes_topics$topicMax<-colnames(textes_topics)[apply(textes_topics,1,which.max)]
textes_topics$text<-rownames(textes_topics)


#### Construction de la base synth√©tique 

tt<-merge(base, textes_topics, by="text") ##ne fonctionne pas √ßa
write.csv(tt,"2023tweet_topic.csv")



## 4. Visualisation des topics
library(LDAvis)

topicmodels2LDAvis <- function(x, ...){
  post <- topicmodels::posterior(x)
  if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
  mat <- x@wordassignments
  LDAvis::createJSON(
    phi = post[["terms"]], 
    theta = post[["topics"]],
    vocab = colnames(post[["terms"]]),
    doc.length = slam::row_sums(mat, na.rm = TRUE),
    term.frequency = slam::col_sums(mat, na.rm = TRUE)
  )
}

lda_js <- topicmodels2LDAvis(res_lda)
serVis(lda_js)
